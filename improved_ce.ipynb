{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improved Stroke Risk Modeling Notebook (Research Prototype)\n",
        "\n",
        "**Warning:** This is a research prototype; NOT for clinical use. External validation and clinical oversight required.\n",
        "\n",
        "## What I did and why (high-level)\n",
        "- Built a fully reproducible ML pipeline with fixed seed (R=42) and version logging.\n",
        "- Automated preprocessing with `ColumnTransformer` and robust handling of numeric/categorical features.\n",
        "- Implemented stratified train/test split to preserve class balance.\n",
        "- Added baselines (Logistic Regression, RandomForest, XGBoost) using pipelines for fair comparison.\n",
        "- Handled class imbalance with SMOTE applied only inside training folds (no leakage) and compared to class_weight.\n",
        "- Tuned hyperparameters via 5-fold Stratified CV optimizing PR AUC (RandomizedSearchCV n_iter=50).\n",
        "- Performed honest evaluation on a held-out test set with ROC/PR curves and confusion matrices.\n",
        "- Optimized decision threshold using out-of-fold predictions to maximize F1.\n",
        "- Assessed probability calibration and optionally calibrated the final model.\n",
        "- Added interpretability: SHAP for tree models and permutation importance.\n",
        "- Saved artifacts (model, CV results, plots) to `/mnt/data/models/` and `/mnt/data/plots/`.\n",
        "\n",
        "Commit message to include:\n",
        "\n",
        "\"Refactor & harden notebook: reproducible preprocessing pipeline, SMOTE-in-CV, CV hyperparameter tuning (RandomizedSearchCV optimizing PR AUC), XGBoost/RandomForest baselines, SHAP explanations, model export, plots and documentation.\"\n",
        "\n",
        "> Run the notebook top-to-bottom to reproduce results. If any ambiguity (e.g., target column) is detected, the notebook will infer and document its choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Pro\\envs\\foml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package installation check complete.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if missing (safe to re-run)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        __import__(pkg.split('==')[0].replace('-', '_'))\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "# Core ML and utilities\n",
        "for p in [\n",
        "    'pandas', 'numpy', 'scikit-learn', 'imbalanced-learn', 'xgboost', 'shap', 'joblib', 'matplotlib', 'seaborn']:\n",
        "    try:\n",
        "        pip_install(p)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not ensure install for {p}: {e}\")\n",
        "\n",
        "print('Package installation check complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Versions:\n",
            "{'pandas': '2.3.3', 'numpy': '2.2.5', 'sklearn': '1.7.2', 'imblearn': '0.14.0', 'xgboost': '3.0.5', 'shap': '0.48.0', 'matplotlib': '3.10.3', 'seaborn': '0.13.2'}\n"
          ]
        }
      ],
      "source": [
        "# Reproducibility: global seeds and version logging\n",
        "import os, random\n",
        "import numpy as np\n",
        "\n",
        "R = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(R)\n",
        "random.seed(R)\n",
        "np.random.seed(R)\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import __version__ as sklearn_version\n",
        "\n",
        "try:\n",
        "    import imblearn\n",
        "    from imblearn import __version__ as imblearn_version\n",
        "except Exception:\n",
        "    imblearn = None\n",
        "    imblearn_version = 'not_installed'\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    from xgboost import __version__ as xgb_version\n",
        "except Exception:\n",
        "    xgb = None\n",
        "    xgb_version = 'not_installed'\n",
        "\n",
        "try:\n",
        "    import shap as shap_pkg\n",
        "    shap_version = shap_pkg.__version__\n",
        "except Exception:\n",
        "    shap_pkg = None\n",
        "    shap_version = 'not_installed'\n",
        "\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "\n",
        "print('Versions:')\n",
        "print({'pandas': pd.__version__, 'numpy': np.__version__, 'sklearn': sklearn_version,\n",
        "       'imblearn': imblearn_version, 'xgboost': xgb_version, 'shap': shap_version,\n",
        "       'matplotlib': matplotlib.__version__, 'seaborn': sns.__version__})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loading and automatic target detection\n",
        "We load the dataset from the workspace if present (e.g., `dataset.csv`), or attempt to infer from any CSV. The target is taken from known names (e.g., `stroke`) or inferred as the only binary column. The choice is documented below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset: dataset.csv\n",
            "Shape: (43400, 12)\n",
            "Columns: ['id', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n",
            "Inferred target column: stroke\n",
            "Class distribution: {0: 42617, 1: 783}\n",
            "Detected numeric columns: 6\n",
            "Detected categorical columns: 5\n"
          ]
        }
      ],
      "source": [
        "# Detect dataset and binary target automatically\n",
        "import glob, json, os\n",
        "import pandas as pd\n",
        "\n",
        "# Try to prefer known dataset names\n",
        "candidate_csvs = []\n",
        "preferred_names = ['dataset.csv', 'data.csv', 'stroke.csv']\n",
        "for name in preferred_names:\n",
        "    if os.path.exists(name):\n",
        "        candidate_csvs.append(name)\n",
        "\n",
        "if not candidate_csvs:\n",
        "    candidate_csvs = glob.glob('*.csv')\n",
        "\n",
        "if not candidate_csvs:\n",
        "    raise FileNotFoundError('No CSV dataset found in working directory. Please add a CSV file (e.g., dataset.csv).')\n",
        "\n",
        "DATA_PATH = candidate_csvs[0]\n",
        "print(f'Using dataset: {DATA_PATH}')\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print('Shape:', df.shape)\n",
        "print('Columns:', list(df.columns))\n",
        "\n",
        "# Try to infer target\n",
        "known_target_candidates = [\n",
        "    'stroke', 'target', 'label', 'outcome', 'y', 'class'\n",
        "]\n",
        "\n",
        "target_col = None\n",
        "for cand in known_target_candidates:\n",
        "    if cand in df.columns:\n",
        "        target_col = cand\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    # Infer binary column: values with exactly 2 unique levels\n",
        "    binary_cols = [c for c in df.columns if df[c].nunique(dropna=False) == 2]\n",
        "    if len(binary_cols) == 1:\n",
        "        target_col = binary_cols[0]\n",
        "    else:\n",
        "        # pick most likely by name containing common keywords\n",
        "        for c in binary_cols:\n",
        "            if any(k in c.lower() for k in ['stroke', 'label', 'target', 'outcome', 'class']):\n",
        "                target_col = c\n",
        "                break\n",
        "\n",
        "if target_col is None:\n",
        "    raise ValueError('Could not infer a unique binary target column. Please add a column named one of: stroke, target, label, outcome, y, class; or ensure a single binary column exists.')\n",
        "\n",
        "print(f'Inferred target column: {target_col}')\n",
        "\n",
        "# Ensure binary encoding 0/1 if needed\n",
        "if df[target_col].dtype == object:\n",
        "    # Map strings to 0/1 by sorting unique values\n",
        "    uniq = sorted(df[target_col].dropna().unique().tolist())\n",
        "    mapping = {uniq[0]: 0, uniq[-1]: 1} if len(uniq) == 2 else None\n",
        "    if mapping is None:\n",
        "        raise ValueError('Target appears non-binary after inference.')\n",
        "    df[target_col] = df[target_col].map(mapping)\n",
        "\n",
        "# Basic class distribution\n",
        "class_counts = df[target_col].value_counts(dropna=False).to_dict()\n",
        "print('Class distribution:', class_counts)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(int)\n",
        "\n",
        "# Report detected dtypes for later pipelines\n",
        "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "print(f'Detected numeric columns: {len(num_cols)}')\n",
        "print(f'Detected categorical columns: {len(cat_cols)}')\n",
        "\n",
        "# Assertions for sanity\n",
        "assert y.nunique() == 2, 'Target must be binary.'\n",
        "assert len(X) == len(y), 'Mismatched X/y length.'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing pipeline\n",
        "We build a robust preprocessing pipeline using `ColumnTransformer`:\n",
        "- Numerics: `SimpleImputer(median)` + `StandardScaler`\n",
        "- Categoricals: `SimpleImputer(most_frequent)` + `OneHotEncoder(handle_unknown='ignore', sparse=False)`\n",
        "\n",
        "This runs inside model pipelines to avoid leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     10\u001b[39m cat_features = user_cat_cols \u001b[38;5;28;01mif\u001b[39;00m user_cat_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cat_cols\n\u001b[32m     12\u001b[39m numeric_transformer = sklearn.pipeline.Pipeline(steps=[\n\u001b[32m     13\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mimputer\u001b[39m\u001b[33m'\u001b[39m, SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m)),\n\u001b[32m     14\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m'\u001b[39m, StandardScaler())\n\u001b[32m     15\u001b[39m ])\n\u001b[32m     17\u001b[39m categorical_transformer = sklearn.pipeline.Pipeline(steps=[\n\u001b[32m     18\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mimputer\u001b[39m\u001b[33m'\u001b[39m, SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmost_frequent\u001b[39m\u001b[33m'\u001b[39m)),\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mencoder\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     20\u001b[39m ])\n\u001b[32m     22\u001b[39m preprocessor = ColumnTransformer(\n\u001b[32m     23\u001b[39m     transformers=[\n\u001b[32m     24\u001b[39m         (\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m, numeric_transformer, num_features),\n\u001b[32m     25\u001b[39m         (\u001b[33m'\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m'\u001b[39m, categorical_transformer, cat_features)\n\u001b[32m     26\u001b[39m     ]\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Quick sanity check on leakage: fit only on train later; here we just define it\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Allow override via variables if user wants to adjust\n",
        "user_num_cols = None\n",
        "user_cat_cols = None\n",
        "\n",
        "num_features = user_num_cols if user_num_cols is not None else num_cols\n",
        "cat_features = user_cat_cols if user_cat_cols is not None else cat_cols\n",
        "\n",
        "numeric_transformer = sklearn.pipeline.Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = sklearn.pipeline.Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features),\n",
        "        ('cat', categorical_transformer, cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Quick sanity check on leakage: fit only on train later; here we just define it\n",
        "print('Preprocessor defined. Numeric features:', len(num_features), 'Categorical features:', len(cat_features))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/test split\n",
        "We split with stratification by the target to preserve class balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=R\n",
        ")\n",
        "\n",
        "print('Train size:', X_train.shape, 'Test size:', X_test.shape)\n",
        "\n",
        "# Assert no leakage and no NaNs post-transform later; for now ensure no issues in raw\n",
        "assert len(X_train) + len(X_test) == len(X)\n",
        "print('Class balance (train):', y_train.value_counts(normalize=True).to_dict())\n",
        "print('Class balance (test):', y_test.value_counts(normalize=True).to_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline pipelines and imbalance handling\n",
        "We use `imblearn.pipeline.Pipeline` to safely include resampling inside CV folds only. We compare:\n",
        "- Logistic Regression (class_weight='balanced')\n",
        "- RandomForestClassifier (with and without class_weight)\n",
        "- XGBClassifier (if available)\n",
        "And SMOTE variants applied within training folds only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define base estimators\n",
        "log_reg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=R)\n",
        "rf = RandomForestClassifier(random_state=R)\n",
        "\n",
        "# Optional XGBoost\n",
        "xgb_est = None\n",
        "if xgb is not None:\n",
        "    try:\n",
        "        from xgboost import XGBClassifier\n",
        "        xgb_est = XGBClassifier(\n",
        "            random_state=R,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print('XGBoost not available:', e)\n",
        "\n",
        "# Pipelines without SMOTE (class_weight or native handling)\n",
        "pipe_lr = ImbPipeline(steps=[('pre', preprocessor), ('clf', log_reg)])\n",
        "pipe_rf = ImbPipeline(steps=[('pre', preprocessor), ('clf', rf)])\n",
        "pipe_xgb = ImbPipeline(steps=[('pre', preprocessor), ('clf', xgb_est)]) if xgb_est is not None else None\n",
        "\n",
        "# Pipelines with SMOTE (only for training folds during CV)\n",
        "# Note: SMOTE will be applied inside CV; we will not fit this pipeline on the test directly without CV\n",
        "smote = SMOTE(random_state=R)\n",
        "pipe_lr_smote = ImbPipeline(steps=[('pre', preprocessor), ('smote', smote), ('clf', LogisticRegression(max_iter=1000, random_state=R))])\n",
        "pipe_rf_smote = ImbPipeline(steps=[('pre', preprocessor), ('smote', smote), ('clf', RandomForestClassifier(random_state=R))])\n",
        "pipe_xgb_smote = ImbPipeline(steps=[('pre', preprocessor), ('smote', smote), ('clf', XGBClassifier(random_state=R, use_label_encoder=False, eval_metric='logloss'))]) if xgb_est is not None else None\n",
        "\n",
        "print('Pipelines prepared (with and without SMOTE).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validation setup and hyperparameter search\n",
        "We use 5-fold Stratified CV with shuffle and fixed seed. Primary scoring is PR AUC (`average_precision`). We record multiple metrics and export `cv_results_` for each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from scipy.stats import loguniform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=R)\n",
        "\n",
        "scoring = {\n",
        "    'average_precision': 'average_precision',\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'f1': 'f1',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall'\n",
        "}\n",
        "\n",
        "param_dists = {}\n",
        "param_dists['lr'] = {\n",
        "    'clf__C': loguniform(1e-4, 1e2),\n",
        "    'clf__penalty': ['l2']\n",
        "}\n",
        "param_dists['rf'] = {\n",
        "    'clf__n_estimators': [100, 200, 400, 800],\n",
        "    'clf__max_depth': [None, 6, 12, 20],\n",
        "    'clf__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "if xgb_est is not None:\n",
        "    param_dists['xgb'] = {\n",
        "        'clf__n_estimators': [100, 200, 400],\n",
        "        'clf__max_depth': [3, 5, 7],\n",
        "        'clf__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'clf__subsample': [0.6, 0.8, 1.0],\n",
        "        'clf__colsample_bytree': [0.5, 0.7, 1.0]\n",
        "    }\n",
        "\n",
        "models = {\n",
        "    'lr': pipe_lr,\n",
        "    'rf': pipe_rf\n",
        "}\n",
        "if pipe_xgb is not None:\n",
        "    models['xgb'] = pipe_xgb\n",
        "\n",
        "# SMOTE variants\n",
        "models_smote = {\n",
        "    'lr_smote': pipe_lr_smote,\n",
        "    'rf_smote': pipe_rf_smote\n",
        "}\n",
        "if pipe_xgb_smote is not None:\n",
        "    models_smote['xgb_smote'] = pipe_xgb_smote\n",
        "\n",
        "os.makedirs('/mnt/data/models', exist_ok=True)\n",
        "\n",
        "cv_results_all = {}\n",
        "\n",
        "for name, model in {**models, **models_smote}.items():\n",
        "    this_param = param_dists.get(name.split('_')[0], {})\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=this_param,\n",
        "        n_iter=50,\n",
        "        scoring='average_precision',\n",
        "        n_jobs=-1,\n",
        "        cv=cv,\n",
        "        refit='average_precision',\n",
        "        verbose=2,\n",
        "        random_state=R,\n",
        "        return_train_score=False\n",
        "    )\n",
        "    print(f'Running CV search for {name}...')\n",
        "    search.fit(X_train, y_train)\n",
        "    # Save results\n",
        "    results_df = pd.DataFrame(search.cv_results_)\n",
        "    results_path = f\"/mnt/data/models/cv_results_{name}.csv\"\n",
        "    results_df.to_csv(results_path, index=False)\n",
        "    print('Saved CV results to', results_path)\n",
        "    cv_results_all[name] = search\n",
        "\n",
        "print('Completed hyperparameter searches.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model selection and test evaluation\n",
        "We pick the single best model by CV `average_precision` and evaluate it on the held-out test set. We report PR AUC, ROC AUC, classification report, confusion matrix, and save PR/ROC plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, classification_report,\n",
        "                             confusion_matrix, precision_recall_curve, roc_curve)\n",
        "\n",
        "# Select best by CV AP on validation\n",
        "best_name, best_search = None, None\n",
        "best_score = -np.inf\n",
        "for name, search in cv_results_all.items():\n",
        "    try:\n",
        "        score = search.best_score_\n",
        "    except Exception:\n",
        "        score = -np.inf\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_name, best_search = name, search\n",
        "\n",
        "print('Best model by CV AP:', best_name, best_score)\n",
        "\n",
        "best_model = best_search.best_estimator_\n",
        "\n",
        "# Fit best on full training set (no leakage: pipelines encapsulate preprocessing/SMOTE accordingly)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on test\n",
        "if hasattr(best_model, 'predict_proba'):\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "elif hasattr(best_model, 'decision_function'):\n",
        "    # scale to 0-1 via min-max for curves\n",
        "    scores = best_model.decision_function(X_test)\n",
        "    smin, smax = scores.min(), scores.max()\n",
        "    y_proba = (scores - smin) / (smax - smin + 1e-12)\n",
        "else:\n",
        "    # fall back to predictions\n",
        "    y_proba = best_model.predict(X_test)\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "pr_auc = average_precision_score(y_test, y_proba)\n",
        "print({'test_roc_auc': roc_auc, 'test_pr_auc': pr_auc})\n",
        "\n",
        "# Curves\n",
        "os.makedirs('/mnt/data/plots', exist_ok=True)\n",
        "\n",
        "prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
        "fpr, tpr, thr_roc = roc_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rec, prec)\n",
        "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR Curve')\n",
        "plt.grid(True)\n",
        "plt.savefig('/mnt/data/plots/pr_curve.png', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve')\n",
        "plt.grid(True)\n",
        "plt.savefig('/mnt/data/plots/roc_curve.png', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Default 0.5 threshold report\n",
        "y_pred_default = (y_proba >= 0.5).astype(int)\n",
        "print('Classification report @0.5 threshold:\\n', classification_report(y_test, y_pred_default, digits=4))\n",
        "cm = confusion_matrix(y_test, y_pred_default)\n",
        "print('Confusion matrix @0.5:\\n', cm)\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix @0.5')\n",
        "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "plt.savefig('/mnt/data/plots/confusion_matrix.png', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "metrics_summary = pd.DataFrame([{'model': best_name, 'test_pr_auc': pr_auc, 'test_roc_auc': roc_auc}])\n",
        "metrics_summary.to_csv('/mnt/data/models/metrics_summary.csv', index=False)\n",
        "print('Saved metrics summary to /mnt/data/models/metrics_summary.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Threshold optimization (maximize F1)\n",
        "We compute out-of-fold (OOF) predicted probabilities on the training set using the selected model configuration to find a threshold that maximizes F1 without peeking at the test set. We then apply that threshold to the test probabilities and report metrics and confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Build an OOF prediction using the best model configuration\n",
        "best_estimator_for_oof = clone(best_search.best_estimator_)\n",
        "\n",
        "oof_probs = np.zeros(len(X_train))\n",
        "for fold_idx, (tr_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "    X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "    est = clone(best_estimator_for_oof)\n",
        "    est.fit(X_tr, y_tr)\n",
        "    if hasattr(est, 'predict_proba'):\n",
        "        oof_probs[val_idx] = est.predict_proba(X_val)[:, 1]\n",
        "    elif hasattr(est, 'decision_function'):\n",
        "        scores = est.decision_function(X_val)\n",
        "        smin, smax = scores.min(), scores.max()\n",
        "        oof_probs[val_idx] = (scores - smin) / (smax - smin + 1e-12)\n",
        "    else:\n",
        "        oof_probs[val_idx] = est.predict(X_val)\n",
        "\n",
        "# Choose threshold maximizing F1 on OOF\n",
        "thr_candidates = np.linspace(0.05, 0.95, 181)\n",
        "best_thr, best_f1 = 0.5, -1\n",
        "for t in thr_candidates:\n",
        "    preds = (oof_probs >= t).astype(int)\n",
        "    score = f1_score(y_train, preds)\n",
        "    if score > best_f1:\n",
        "        best_f1 = score\n",
        "        best_thr = t\n",
        "\n",
        "print({'best_threshold_oof': best_thr, 'oof_f1': best_f1})\n",
        "\n",
        "# Apply on test\n",
        "y_pred_best = (y_proba >= best_thr).astype(int)\n",
        "print('Classification report @optimized threshold:\\n', classification_report(y_test, y_pred_best, digits=4))\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "print('Confusion matrix @optimized threshold:\\n', cm_best)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calibration\n",
        "We visualize calibration and, if miscalibrated, optionally fit a calibrated model with isotonic or sigmoid calibration and compare performance on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.calibration import CalibrationDisplay, CalibratedClassifierCV\n",
        "\n",
        "# Plot calibration of best model\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "CalibrationDisplay.from_estimator(best_model, X_test, y_test, ax=ax)\n",
        "plt.title('Calibration Curve (Best Model)')\n",
        "plt.savefig('/mnt/data/plots/calibration_curve.png', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Optional: Fit calibrated version and compare quickly (wrapped with try/except for speed)\n",
        "try:\n",
        "    calibrated = CalibratedClassifierCV(best_model, method='isotonic', cv=3)\n",
        "    calibrated.fit(X_train, y_train)\n",
        "    if hasattr(calibrated, 'predict_proba'):\n",
        "        y_proba_cal = calibrated.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        scores = calibrated.decision_function(X_test)\n",
        "        smin, smax = scores.min(), scores.max()\n",
        "        y_proba_cal = (scores - smin) / (smax - smin + 1e-12)\n",
        "    roc_auc_cal = roc_auc_score(y_test, y_proba_cal)\n",
        "    pr_auc_cal = average_precision_score(y_test, y_proba_cal)\n",
        "    print({'test_roc_auc_calibrated': roc_auc_cal, 'test_pr_auc_calibrated': pr_auc_cal})\n",
        "except Exception as e:\n",
        "    print('Calibration step skipped:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretability with SHAP and permutation importance\n",
        "We compute SHAP values for the final tree-based model. Explanations are computed on preprocessed features (post-`ColumnTransformer`). For categorical variables, one-hot features are grouped back to original names in reporting. We also compute permutation importance as a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Only attempt SHAP for tree-based models\n",
        "is_tree = any(k in best_name for k in ['rf', 'xgb'])\n",
        "\n",
        "try:\n",
        "    # Build a preprocessed training sample for SHAP background\n",
        "    from sklearn import set_config\n",
        "    set_config(transform_output='pandas')\n",
        "    X_train_pre = preprocessor.fit_transform(X_train)\n",
        "    X_test_pre = preprocessor.transform(X_test)\n",
        "    feature_names = X_train_pre.columns.tolist()\n",
        "except Exception:\n",
        "    # Fallback to numpy arrays\n",
        "    set_config(transform_output=None)\n",
        "    X_train_pre = preprocessor.fit_transform(X_train)\n",
        "    X_test_pre = preprocessor.transform(X_test)\n",
        "    feature_names = [f'f_{i}' for i in range(X_train_pre.shape[1])]\n",
        "\n",
        "if is_tree and shap_pkg is not None:\n",
        "    try:\n",
        "        # Extract the underlying tree model from the pipeline\n",
        "        final_clf = best_model.named_steps['clf']\n",
        "        # Background subset to speed up\n",
        "        rng = np.random.RandomState(R)\n",
        "        bg_idx = rng.choice(np.arange(X_train_pre.shape[0]), size=min(500, X_train_pre.shape[0]), replace=False)\n",
        "        background = X_train_pre[bg_idx]\n",
        "\n",
        "        explainer = shap_pkg.TreeExplainer(final_clf)\n",
        "        shap_values = explainer.shap_values(X_test_pre)\n",
        "\n",
        "        # Handle binary shap output shape differences\n",
        "        if isinstance(shap_values, list) and len(shap_values) == 2:\n",
        "            shap_values_pos = shap_values[1]\n",
        "        else:\n",
        "            shap_values_pos = shap_values\n",
        "\n",
        "        # Summary plot\n",
        "        plt.figure()\n",
        "        shap_pkg.summary_plot(shap_values_pos, X_test_pre, show=False, feature_names=feature_names)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/mnt/data/plots/shap_summary.png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Top features for dependence plots\n",
        "        mean_abs = np.abs(shap_values_pos).mean(axis=0)\n",
        "        top_idx = np.argsort(-mean_abs)[:3]\n",
        "        for i in top_idx:\n",
        "            fname = feature_names[i] if i < len(feature_names) else f'feat_{i}'\n",
        "            shap_pkg.dependence_plot(i, shap_values_pos, X_test_pre, show=False, feature_names=feature_names)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'/mnt/data/plots/shap_dependence_{fname}.png', bbox_inches='tight')\n",
        "            plt.close()\n",
        "        print('Saved SHAP plots.')\n",
        "    except Exception as e:\n",
        "        print('SHAP step skipped:', e)\n",
        "else:\n",
        "    print('SHAP skipped: final model not tree-based or shap not available.')\n",
        "\n",
        "# Permutation importance as sanity check\n",
        "try:\n",
        "    pi = permutation_importance(best_model, X_test, y_test, n_repeats=5, random_state=R, scoring='average_precision')\n",
        "    pi_df = pd.DataFrame({'feature': X.columns, 'importance_mean': pi.importances_mean[:len(X.columns)]})\n",
        "    pi_df.sort_values('importance_mean', ascending=False).to_csv('/mnt/data/models/permutation_importance.csv', index=False)\n",
        "    print('Saved permutation importance to /mnt/data/models/permutation_importance.csv')\n",
        "except Exception as e:\n",
        "    print('Permutation importance skipped:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save artifacts\n",
        "We save the final pipeline, metrics, and CV results into `/mnt/data/models/` and plots into `/mnt/data/plots/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib, os, json\n",
        "\n",
        "os.makedirs('/mnt/data/models', exist_ok=True)\n",
        "os.makedirs('/mnt/data/plots', exist_ok=True)\n",
        "\n",
        "# Save best pipeline\n",
        "pipeline_path = '/mnt/data/models/best_pipeline.joblib'\n",
        "joblib.dump(best_model, pipeline_path)\n",
        "print('Saved final pipeline to', pipeline_path)\n",
        "\n",
        "# Save best params summary\n",
        "best_summary = {\n",
        "    'best_model': best_name,\n",
        "    'best_score_average_precision_cv': float(best_score),\n",
        "    'best_params': best_search.best_params_,\n",
        "    'test_pr_auc': float(pr_auc),\n",
        "    'test_roc_auc': float(roc_auc)\n",
        "}\n",
        "with open('/mnt/data/models/best_summary.json', 'w') as f:\n",
        "    json.dump(best_summary, f, indent=2)\n",
        "print('Saved best summary to /mnt/data/models/best_summary.json')\n",
        "\n",
        "# Confirm files exist\n",
        "print('Artifacts in /mnt/data/models:', os.listdir('/mnt/data/models'))\n",
        "print('Artifacts in /mnt/data/plots:', os.listdir('/mnt/data/plots'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original outputs snapshot\n",
        "If an original notebook exists, we snapshot outputs for comparison. Otherwise, we skip gracefully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "orig_path = '/mnt/data/108543c7-ab18-4c4e-8d63-26f3e5e622ce.ipynb'\n",
        "snapshot_path = '/mnt/data/models/original_outputs_snapshot.ipynb'\n",
        "try:\n",
        "    if os.path.exists(orig_path):\n",
        "        shutil.copy2(orig_path, snapshot_path)\n",
        "        print('Saved original notebook snapshot to', snapshot_path)\n",
        "    else:\n",
        "        print('Original notebook not found; skipping snapshot.')\n",
        "except Exception as e:\n",
        "    print('Snapshot failed:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final summary, results, and README-style instructions\n",
        "- Primary metric: **PR AUC** due to class imbalance.\n",
        "- We compared LR, RF, XGB (with/without SMOTE-in-CV) and tuned via `RandomizedSearchCV` with 5-fold Stratified CV.\n",
        "- The best model by CV PR AUC is reported with test PR AUC and ROC AUC.\n",
        "- We optimized the decision threshold on OOF predictions to maximize F1, then evaluated on test.\n",
        "- Calibration was plotted; optional isotonic calibration compared.\n",
        "- SHAP explanations (for tree models) and permutation importance were generated.\n",
        "- All artifacts saved under `/mnt/data/models/` and `/mnt/data/plots/`.\n",
        "\n",
        "#### Reproduce\n",
        "1. Run all cells from top to bottom. Ensure a CSV dataset is present (e.g., `dataset.csv`).\n",
        "2. The notebook will auto-detect the target (e.g., `stroke`) or infer a binary column.\n",
        "3. Find outputs under `/mnt/data/models/` and `/mnt/data/plots/`.\n",
        "\n",
        "#### Ethics/caveats\n",
        "- This is a research prototype; NOT for clinical use. External validation and clinical oversight required.\n",
        "- Dataset shifts, sampling bias, and mislabeled data can mislead metrics.\n",
        "- Never use this model for clinical decisions without rigorous validation.\n",
        "\n",
        "A compact results table and saved pipeline path are printed above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "foml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
